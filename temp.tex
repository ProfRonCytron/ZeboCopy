\SetTitle{99}{temp}{faster}{99}



\begin{frame}{Our example revisited}{We use logic to determine the bits of $s$}

\begin{itemize}
    \item We consider $s=s_{1}s_{2}s_{3}$ so we can reason about the bits of $s$ separately.  Since $s$ does not necessarily denote a binary numerical value, we are free to number these left-to-right, as shown.
    \item Each observable value $w$ can provide clues about the bits of $s$, as follows:
\begin{description}
   \item[010]  Because $\DotP{010}{s_{1}s_{2}s_{3}}=0$, we can conclude $s_{2}=0$.
   \item[101]  We now require \[
   \Implies{\DotP{101}{s_{1}0s_{3}}=0}{\Xor{s_1}{s_3}=0}\]
   Thus $s_1$ and $s_3$ are either both~$0$ or both~$1$.
\end{description}
   \item Since $s=000$ is not allowed, we must have the latter case.  
   \item This yields the correct answer, $s=101$
\end{itemize}

    
\end{frame}
{
\def\L#1{\raisebox{-0.65em}{#1}}
\begin{frame}{Formalizing this logic}{Using Gaussian elimination}
\begin{center}
\begin{tabular}{rclcc}
\begin{DotPBox}{3}{1}{3}
\Qbit{0}{1}
\Qbit{1}{1}
\Qbit{2}{0}
\end{DotPBox} & & \begin{DotPBox}{1}{1}{1}\Qbit{0}{s_1}\end{DotPBox}  & & \L{0}\\
\begin{DotPBox}{3}{1}{3}
\Qbit{0}{1}
\Qbit{1}{0}
\Qbit{2}{1}
\end{DotPBox}& \L{\FCirc{0.3}} & \begin{DotPBox}{1}{1}{1}\Qbit{0}{s_2}\end{DotPBox}  &\L{=}& \L{0}\\
\begin{DotPBox}{3}{1}{3}
\Qbit{0}{1}
\Qbit{1}{1}
\Qbit{2}{0}
\end{DotPBox}& & \begin{DotPBox}{1}{1}{1}\Qbit{0}{s_3}\end{DotPBox} & & \L{0}
\end{tabular}
\end{center}
\begin{itemize}
    \item Each $w$ is a row of the above matrix, and its dot product with the column vector must be $0$.
    \item To obtain a solution, we need $n$ linearly independent rows.
    \item Performing \href{https://en.wikipedia.org/wiki/Gaussian_elimination}{Gaussian elimination} on the system to obtain $s$ takes $\Theta(n^{3})$ time using the most common algorithm.  Multiplication and addition are \And{}{} and \Xor{}{} here.
\end{itemize}
    
\end{frame}
}